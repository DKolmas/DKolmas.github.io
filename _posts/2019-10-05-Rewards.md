---
title: "Reward and Returns in Reinforcement Learning"
date: 2019-10-05
categories: [RL]
tags: [Reinforcement Learning, Theory]
excerpt: "What is reward and return? It is a foundation when introducing concept of value function"
mathjax: "true"
order: 6
---

> <span style="color:dodgerblue">**Take away message from the note:**</span>
> * <span style="color:dodgerblue">**Reward is a response of an environment to the action at a single time step $$t$$**</span>
> * <span style="color:dodgerblue">**Return is a sum of all rewards**</span>
> * <span style="color:dodgerblue">**Return typicaly incorporates discount factor and can be shown in following form: $$G_t=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}$$**</span>
> * <span style="color:dodgerblue">**The goal of RL agent is to learn a policy that maximizes return**</span>

Last update: 24th of Janauary, 2020

### What you can find in this note?
1. What is reward and return?
2. Return with discounted factor
3. Additional notes on return

### What is reward and return?

In reinforcement learning framework, at each time step $$t$$ agent takes action and **environment responses back with special signal called reward** $$R_t\in\mathbb{R}$$. A reward is a simple number (value). Informally, the goal of the agent is to maximize the total amount of rewards it receives from the environment. Therefore it is not important to maximize imeediate reward but total sum of all rewards (cumulative reward) over long run. This concept can be stated in a followed way as **reward hypothesis**:

That all of what we mean by goals and purposes can be well thought as a **maximization of the expected value** of the cumulatvie sum of received scalar signal (called reward).

We can present a sequence of rewards received after time step $$t$$ as:
$$ G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + ... R_T$$

where $$T$$ is the final step. In reinformcement learning we want to maximize expected value of **return G_t**. In the above case the return is presented as a function where all rewards are summed up. This function is the simplest one. There might be other functions to translate indivdual rewards into return we want to maximimze.

The above definition of return is applicable to episodic tasks. Episodic task are the one in which a sequence of states, actions and rewards breaks naturally into episodes (as games for example). As opposed to epsiodic task we have continous task which are those that cannot be broken naturaly into episodes (for example controlling some process in power plant).

### Return with discounted factor

While calculating return we often use doscounted factor so the formula takes following form:

$$G_t=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}$$ 

Discount factor $$\gamma^k$$ here is to show current value (at the time step $$t$$) of future reward. $$\gamma$$ takes value in the range between 0 and 1. $$\gamma=$$1 indicates that the value of rewards from the future have the same value now, at the time step $$t$$.

For $$\gamma<$$1 and $$R_t=1$$ return can take simplified form

$$G_t=\sum_{k=0}^{\infty}\gamma^k=\frac{1}{1-\gamma}$$

### Additional notes on returns

Recursive relatinship of return $$G_t$$ and return in following time step $$G_{t+1}$$ is very important in reinforcement learning. Return in successive time steps are related to each other in a way shown below:

![image](/images/returns_recursive_relationship.png)

The same type of recursive relation as shown above will be also used when [Bellman Eqution](http://www.damiankolmas.com/rl/Bellman-Equations-Introduction/#) is discussed for value functions.
